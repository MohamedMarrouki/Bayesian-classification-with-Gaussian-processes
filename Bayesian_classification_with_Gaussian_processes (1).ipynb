{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28049596",
      "metadata": {
        "id": "28049596"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a95b837a",
      "metadata": {
        "id": "a95b837a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.impute import SimpleImputer\n",
        "import requests\n",
        "from io import StringIO\n",
        "import GPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dqSdZJnLQtOg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqSdZJnLQtOg",
        "outputId": "8bcefe42-8985-46db-95d3-8f8bfcec9510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy version: 1.25.2\n",
            "SciPy version: 1.11.1\n",
            "Scikit-learn version: 1.3.0\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import scipy\n",
        "import sklearn\n",
        "print(f\"NumPy version: {numpy.__version__}\")\n",
        "print(f\"SciPy version: {scipy.__version__}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7d8036",
      "metadata": {
        "id": "7f7d8036"
      },
      "source": [
        "## Data Load (PIMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b21f83",
      "metadata": {
        "id": "f4b21f83"
      },
      "outputs": [],
      "source": [
        "def load_pima_dataset_full_features():\n",
        "    url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "    col_names = ['pregnancies', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
        "    df = pd.read_csv(url, header=None, names=col_names)\n",
        "    \n",
        "    feature_names = col_names[:-1]\n",
        "    X = df[feature_names].values\n",
        "    y = df['label'].values\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    \n",
        "    for col in ['glucose', 'bp', 'skin', 'insulin', 'bmi']:\n",
        "        col_index = col_names.index(col)\n",
        "        non_zero_values = X[:, col_index][X[:, col_index] != 0]\n",
        "        if non_zero_values.size > 0:\n",
        "            mean_val = non_zero_values.mean()\n",
        "            X[:, col_index][X[:, col_index] == 0] = mean_val\n",
        "        else:\n",
        "            X[:, col_index][X[:, col_index] == 0] = 0 \n",
        "    \n",
        "    return X, y, feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdd47f2",
      "metadata": {
        "id": "8cdd47f2"
      },
      "source": [
        "## Data Load (CRABS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b50c4eb3",
      "metadata": {
        "id": "b50c4eb3"
      },
      "outputs": [],
      "source": [
        "def load_crabs_dataset_all_features():\n",
        "    url = \"https://raw.githubusercontent.com/fernandomayer/data/master/crabs.csv\"\n",
        "    \n",
        "    df = pd.read_csv(\n",
        "        url,\n",
        "        sep=';',           # Use semicolon as the delimiter\n",
        "        header=0,          # The first row is the header\n",
        "        decimal=',',       # Use comma as the decimal separator\n",
        "        quotechar='\"',     # Handle double quotes around fields\n",
        "        engine='python'    # Python engine for robustness with custom separators/quotes\n",
        "    )\n",
        "\n",
        "    df = df.rename(columns={'especie': 'sp', 'sexo': 'sex'})\n",
        "\n",
        "    df['sp'] = df['sp'].astype(str).str.strip()\n",
        "    df['sex'] = df['sex'].astype(str).str.strip()\n",
        "    \n",
        "    print(f\"Crabs dataset: Unique 'sp' values before filtering: {df['sp'].unique()}\")\n",
        "\n",
        "    df = df[df['sp'].isin(['azul', 'laranja'])]\n",
        "    \n",
        "    if df.empty:\n",
        "        raise ValueError(\n",
        "            \"DataFrame is empty after filtering 'sp' column. \"\n",
        "            \"This indicates 'azul' or 'laranja' species might not be present or are malformed.\"\n",
        "        )\n",
        "\n",
        "    feature_columns = ['FL', 'RW', 'CL', 'CW', 'BD']\n",
        "\n",
        "    for col in feature_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    \n",
        "    X = df[feature_columns].values\n",
        "    \n",
        "    if np.isnan(X).any():\n",
        "        print(\"Warning: NaNs found in Crabs features. Imputing with mean...\")\n",
        "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "        X = imputer.fit_transform(X)\n",
        "\n",
        "    y = np.where(df['sp'] == 'azul', 1, -1)\n",
        "    \n",
        "    return X, y, feature_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5827c921",
      "metadata": {
        "id": "5827c921"
      },
      "source": [
        "## RBF Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09cd4bf",
      "metadata": {
        "id": "f09cd4bf"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel_ard(X1, X2, length_scales, sigma_f=1.0):\n",
        "    length_scales = np.asarray(length_scales).flatten()\n",
        "    if len(length_scales) != X1.shape[1]:\n",
        "        raise ValueError(\"Number of length scales must match number of features.\")\n",
        "    \n",
        "    sqdist = np.sum((X1[:, np.newaxis, :] - X2[np.newaxis, :, :])**2 / length_scales**2, axis=-1)\n",
        "    # Augmenting jitter for numerical stability\n",
        "    K = sigma_f**2 * np.exp(-0.5 * sqdist)\n",
        "    if X1.shape[0] == X2.shape[0] and np.array_equal(X1, X2):\n",
        "        K += 1e-4 * np.eye(X1.shape[0]) # Increased jitter\n",
        "    return K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db4d073",
      "metadata": {
        "id": "5db4d073"
      },
      "source": [
        "## Sigmoid Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84bf54f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    x_clamped = np.clip(x, -500, 500) \n",
        "    return np.where(x_clamped >= 0, 1 / (1 + np.exp(-x_clamped)), np.exp(x_clamped) / (1 + np.exp(x_clamped)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49eb4dd7",
      "metadata": {
        "id": "49eb4dd7"
      },
      "source": [
        "## Laplace Approximation For GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f30acc7",
      "metadata": {
        "id": "9f30acc7"
      },
      "outputs": [],
      "source": [
        "def laplace_approximation_and_marginal_likelihood(X, y, kernel_func, length_scales, sigma_f):\n",
        "    n = X.shape[0]\n",
        "    K = kernel_func(X, X, length_scales, sigma_f)\n",
        "    \n",
        "    f = np.zeros(n)\n",
        "    for _ in range(100): \n",
        "        pi = sigmoid(y * f)\n",
        "        W = pi * (1 - pi)\n",
        "        \n",
        "        W[W < 1e-10] = 1e-10 \n",
        "        W_sqrt = np.sqrt(W)\n",
        "        \n",
        "        grad = y * (pi - 1)\n",
        "        \n",
        "        try:\n",
        "            B = np.eye(n) + np.diag(W_sqrt) @ K @ np.diag(W_sqrt)\n",
        "            L = np.linalg.cholesky(B)\n",
        "        except np.linalg.LinAlgError:\n",
        "            K_stable = K + 1e-3 * np.eye(n) # More aggressive jitter if Cholesky fails\n",
        "            B = np.eye(n) + np.diag(W_sqrt) @ K_stable @ np.diag(W_sqrt)\n",
        "            L = np.linalg.cholesky(B)\n",
        "\n",
        "        b = W * f + grad\n",
        "        \n",
        "        try:\n",
        "            v_intermediate = W_sqrt * (K @ b) \n",
        "            v_solve = np.linalg.solve(L, v_intermediate) \n",
        "            a_term = np.linalg.solve(L.T, v_solve) \n",
        "            a = b - W_sqrt * a_term\n",
        "            \n",
        "        except np.linalg.LinAlgError as e:\n",
        "            f_hat_on_error = np.zeros(n) \n",
        "            return f_hat_on_error, 1e10, K, W_sqrt, L \n",
        "\n",
        "        f_hat = K @ a\n",
        "        \n",
        "        if np.linalg.norm(f - f_hat) < 1e-7 or np.any(np.abs(f_hat) > 1e15): \n",
        "            break\n",
        "        f = f_hat\n",
        "    \n",
        "    pi_hat = sigmoid(y * f_hat)\n",
        "    pi_hat[pi_hat <= 0] = 1e-10 \n",
        "    log_p_y_given_f = np.sum(np.log(pi_hat))\n",
        "    \n",
        "    neg_log_marginal_likelihood = 0.5 * f_hat.T @ np.linalg.solve(K, f_hat) + \\\n",
        "                                  np.sum(np.log(np.diag(L))) - \\\n",
        "                                  log_p_y_given_f\n",
        "    \n",
        "    return f_hat, neg_log_marginal_likelihood, K, W_sqrt, L"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7080157",
      "metadata": {
        "id": "a7080157"
      },
      "source": [
        "## Prediction with GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fcb1a7d",
      "metadata": {
        "id": "5fcb1a7d"
      },
      "outputs": [],
      "source": [
        "def predict_gp(X_test, X_train, y_train, K, f_hat, W_sqrt, L, kernel_func, length_scales, sigma_f):\n",
        "    K_s = kernel_func(X_train, X_test, length_scales, sigma_f)\n",
        "    \n",
        "    v = np.linalg.solve(L, W_sqrt[:, np.newaxis] * K_s) \n",
        "    \n",
        "    mu_f_s = K_s.T @ (y_train * (sigmoid(y_train * f_hat) - 1))\n",
        "\n",
        "    K_ss = kernel_func(X_test, X_test, length_scales, sigma_f)\n",
        "    \n",
        "    var_f_s = np.diag(K_ss) - np.sum(v**2, axis=0)\n",
        "    \n",
        "    var_f_s[var_f_s < 0] = 0\n",
        "    \n",
        "    denominator_term = 1 + np.pi * var_f_s / 8\n",
        "    prob = sigmoid(mu_f_s / np.sqrt(denominator_term + 1e-10))\n",
        "    \n",
        "    return prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b791f84d",
      "metadata": {
        "id": "b791f84d"
      },
      "source": [
        "## Classification Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941e3289",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_classification_pipeline(dataset_loader_func, visualize=False):\n",
        "    X, y, feature_names = dataset_loader_func()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    dataset_name = dataset_loader_func.__name__.replace(\"load_\", \"\").replace(\"_dataset\", \"\").replace(\"_full_features\", \"\").replace(\"_all_features\", \"\").title()\n",
        "    print(f\"================== {dataset_name} Dataset ==================\")\n",
        "    print(f\"Training on {X_train.shape[0]} samples, testing on {X_test.shape[0]} samples.\")\n",
        "    print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "    def gp_objective(params):\n",
        "        min_log_val_length_scales = -6.0\n",
        "        max_log_val_length_scales = 6.0\n",
        "        min_log_val_sigma_f = -3.5 # Adjusted to try to find better sigma_f\n",
        "        max_log_val_sigma_f = 6.0\n",
        "\n",
        "        length_scales = np.exp(params[:-1])\n",
        "        sigma_f = np.exp(params[-1])\n",
        "\n",
        "        if np.any(length_scales < 1e-4) or sigma_f < 1e-4:\n",
        "             return 1e9\n",
        "\n",
        "        try:\n",
        "            f_hat, neg_log_ml, _, _, _ = laplace_approximation_and_marginal_likelihood(\n",
        "                X_train, y_train, rbf_kernel_ard, length_scales, sigma_f\n",
        "            )\n",
        "            if np.all(f_hat == 0) and neg_log_ml > 1e8:\n",
        "                return 1e9\n",
        "            if np.isnan(neg_log_ml) or np.isinf(neg_log_ml):\n",
        "                return 1e9\n",
        "            return neg_log_ml\n",
        "        except np.linalg.LinAlgError:\n",
        "            return 1e9\n",
        "        except ValueError as e:\n",
        "            return 1e9\n",
        "\n",
        "    num_features = X_train.shape[1]\n",
        "    \n",
        "    num_restarts = 50\n",
        "    best_neg_log_ml = np.inf\n",
        "    best_optimized_params = None\n",
        "\n",
        "    print(\"\\n--- Training Gaussian Process Model (with restarts) ---\")\n",
        "    print(f\"Optimizing GP hyperparameters with {num_restarts} random restarts...\")\n",
        "\n",
        "    search_bounds = [(-6.0, 6.0)] * num_features + [(-3.5, 6.0)]\n",
        "    optimizer_options = {'maxiter': 2000}\n",
        "\n",
        "    for i in range(num_restarts):\n",
        "        random_initial_params = np.random.uniform(-4.0, 4.0, num_features + 1)\n",
        "        random_initial_params[:-1] = np.clip(random_initial_params[:-1], search_bounds[0][0], search_bounds[0][1])\n",
        "        random_initial_params[-1] = np.clip(random_initial_params[-1], search_bounds[-1][0], search_bounds[-1][1])\n",
        "\n",
        "        current_res = minimize(gp_objective, random_initial_params, bounds=search_bounds, \n",
        "                               method='L-BFGS-B', options=optimizer_options)\n",
        "        \n",
        "        current_neg_log_ml = current_res.fun\n",
        "\n",
        "        if current_neg_log_ml < best_neg_log_ml:\n",
        "            best_neg_log_ml = current_neg_log_ml\n",
        "            best_optimized_params = current_res.x\n",
        "\n",
        "    if best_optimized_params is None:\n",
        "        raise RuntimeError(\"GP optimization failed to find any valid parameters after all restarts.\")\n",
        "\n",
        "    optimized_params = best_optimized_params\n",
        "    optimized_params[:-1] = np.clip(optimized_params[:-1], search_bounds[0][0], search_bounds[0][1])\n",
        "    optimized_params[-1] = np.clip(optimized_params[-1], search_bounds[-1][0], search_bounds[-1][1])\n",
        "    \n",
        "    optimized_length_scales = np.exp(optimized_params[:-1])\n",
        "    optimized_sigma_f = np.exp(optimized_params[-1])\n",
        "\n",
        "    print(\"GP Optimization complete.\")\n",
        "    print(f\"  - Final Optimal sigma_f: {optimized_sigma_f:.3f}\")\n",
        "    for i, ls in enumerate(optimized_length_scales):\n",
        "        if i < len(feature_names):\n",
        "            print(f\"  - Final Optimal length_scale for '{feature_names[i]}': {ls:.3f}\")\n",
        "        else:\n",
        "            print(f\"  - Final Optimal length_scale {i}: {ls:.3f} (No corresponding feature name)\")\n",
        "\n",
        "    try:\n",
        "        f_hat, _, K, W_sqrt, L = laplace_approximation_and_marginal_likelihood(\n",
        "            X_train, y_train, rbf_kernel_ard, optimized_length_scales, optimized_sigma_f\n",
        "        )\n",
        "        gp_probs = predict_gp(X_test, X_train, y_train, K, f_hat, W_sqrt, L, rbf_kernel_ard, optimized_length_scales, optimized_sigma_f)\n",
        "        gp_y_pred = np.where(gp_probs > 0.5, 1, -1)\n",
        "        gp_accuracy = accuracy_score(y_test, gp_y_pred)\n",
        "        print(f\"\\n>>> GP Test Accuracy: {gp_accuracy:.4f} <<<\")\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"\\nGP prediction failed due to LinAlgError. Accuracy cannot be computed.\")\n",
        "        gp_accuracy = np.nan\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during GP prediction: {e}. Accuracy cannot be computed.\")\n",
        "        gp_accuracy = np.nan\n",
        "\n",
        "    if visualize and X_train.shape[1] == 2:\n",
        "        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "        grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "        \n",
        "        grid_points_scaled = scaler.transform(grid_points)\n",
        "\n",
        "        Z = predict_gp(grid_points_scaled, X_train, y_train, K, f_hat, W_sqrt, L, rbf_kernel_ard, optimized_length_scales, optimized_sigma_f)\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        contour = plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm, levels=np.linspace(0, 1, 11))\n",
        "        plt.colorbar(contour, label='GP Probability of Class +1')\n",
        "        plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
        "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', marker='o', s=80, label='Train Data')\n",
        "        plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm, edgecolors='k', marker='x', s=80, label='Test Data')\n",
        "\n",
        "        plt.xlabel(f\"{feature_names[0]} (Standardized)\")\n",
        "        plt.ylabel(f\"{feature_names[1]} (Standardized)\")\n",
        "        plt.title(f'GP Classification Results for {dataset_name} Dataset')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    \n",
        "    print(\"\\n--- Training Support Vector Machine (SVM) Model for Comparison ---\")\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': [1, 0.1, 0.01, 0.001],\n",
        "        'kernel': ['rbf']\n",
        "    }\n",
        "    \n",
        "    grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0, cv=5)\n",
        "    print(\"Running GridSearchCV for SVM...\")\n",
        "    grid.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"SVM GridSearch complete.\")\n",
        "    print(f\"  - Best SVM Parameters: {grid.best_params_}\")\n",
        "    \n",
        "    svm_y_pred = grid.predict(X_test)\n",
        "    svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "    print(f\"\\n>>> SVM Test Accuracy: {svm_accuracy:.4f} <<<\")\n",
        "    print(\"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b33f5e6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b33f5e6e",
        "outputId": "b2780164-da72-4231-8be3-28dc462f18fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================== Pima Dataset ==================\n",
            "Training on 537 samples, testing on 231 samples.\n",
            "Number of features: 8\n",
            "\n",
            "--- Training Gaussian Process Model (with restarts) ---\n",
            "Optimizing GP hyperparameters with 50 random restarts...\n",
            "GP Optimization complete.\n",
            "  - Final Optimal sigma_f: 0.030\n",
            "  - Final Optimal length_scale for 'pregnancies': 0.021\n",
            "  - Final Optimal length_scale for 'glucose': 403.429\n",
            "  - Final Optimal length_scale for 'bp': 0.320\n",
            "  - Final Optimal length_scale for 'skin': 0.012\n",
            "  - Final Optimal length_scale for 'insulin': 0.002\n",
            "  - Final Optimal length_scale for 'bmi': 41.546\n",
            "  - Final Optimal length_scale for 'pedigree': 0.032\n",
            "  - Final Optimal length_scale for 'age': 66.989\n",
            "\n",
            ">>> GP Test Accuracy: 0.4935 <<<\n",
            "\n",
            "--- Training Support Vector Machine (SVM) Model for Comparison ---\n",
            "Running GridSearchCV for SVM...\n",
            "SVM GridSearch complete.\n",
            "  - Best SVM Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            ">>> SVM Test Accuracy: 0.7446 <<<\n",
            "============================================================\n",
            "\n",
            "Crabs dataset: Unique 'sp' values before filtering: ['azul' 'laranja']\n",
            "Warning: NaNs found in Crabs features. Imputing with mean...\n",
            "================== Crabs Dataset ==================\n",
            "Training on 109 samples, testing on 47 samples.\n",
            "Number of features: 5\n",
            "\n",
            "--- Training Gaussian Process Model (with restarts) ---\n",
            "Optimizing GP hyperparameters with 50 random restarts...\n",
            "GP Optimization complete.\n",
            "  - Final Optimal sigma_f: 0.030\n",
            "  - Final Optimal length_scale for 'FL': 403.429\n",
            "  - Final Optimal length_scale for 'RW': 173.054\n",
            "  - Final Optimal length_scale for 'CL': 280.066\n",
            "  - Final Optimal length_scale for 'CW': 204.184\n",
            "  - Final Optimal length_scale for 'BD': 403.429\n",
            "\n",
            ">>> GP Test Accuracy: 0.5319 <<<\n",
            "\n",
            "--- Training Support Vector Machine (SVM) Model for Comparison ---\n",
            "Running GridSearchCV for SVM...\n",
            "SVM GridSearch complete.\n",
            "  - Best SVM Parameters: {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
            "\n",
            ">>> SVM Test Accuracy: 0.9787 <<<\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_classification_pipeline(load_pima_dataset_full_features, visualize=False)\n",
        "run_classification_pipeline(load_crabs_dataset_all_features, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35949a87",
      "metadata": {
        "id": "35949a87"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "asienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
